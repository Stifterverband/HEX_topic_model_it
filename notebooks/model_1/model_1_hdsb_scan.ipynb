{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d2dc7e",
   "metadata": {},
   "source": [
    "# Template für Topic Modeling\n",
    "Dieses Template soll dabei helfen, Topic Modeling automatisiert und einheitlich durchzuführen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3a5f4",
   "metadata": {},
   "source": [
    "## Allgemeine Vorbereitungsschritte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634674f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktuelles Arbeitsverzeichnis anzeigen und bei Bedarf anpassen\n",
    "# print(os.getcwd())\n",
    "# os.chdir(\"C:/SV/HEX/Topic Modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d61603",
   "metadata": {},
   "source": [
    "### Pakete laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1471af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktuelles Arbeitsverzeichnis anzeigen und bei Bedarf anpassen\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"C:/Users/mhu/Documents/github/topic_model_it/\")\n",
    "\n",
    "import pandas as pd\n",
    "import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from bertopic.vectorizers import ClassTfidfTransformer \n",
    "import openpyxl\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feeb504",
   "metadata": {},
   "source": [
    "### Seed setzen\n",
    "Wir setzen einen festen Seed, um Zufallszahlen in NumPy und PyTorch reproduzierbar zu machen, sowohl auf der CPU als auch auf der GPU (falls verfügbar). Das stellt sicher, dass Berechnungen mit zufälligen Operationen bei wiederholter Ausführung dieselben Ergebnisse liefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d700e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 40  # Initialisiert den Seed-Wert für reproduzierbare Ergebnisse\n",
    "np.random.seed(seed)  # Setzt den Seed für NumPy-Zufallszahlengeneratoren\n",
    "random.seed(seed)  # Setzt den Seed für den Python-eigenen Zufallszahlengenerator\n",
    "torch.manual_seed(seed)  # Setzt den Seed für PyTorch-Zufallszahlen\n",
    "if torch.cuda.is_available():  # Überprüft, ob CUDA (GPU-Unterstützung) verfügbar ist\n",
    "    torch.cuda.manual_seed_all(seed)  # Setzt den Seed für alle CUDA-Zufallszahlen (für GPU-Berechnungen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99ec26",
   "metadata": {},
   "source": [
    "### Datensätze einlesen\n",
    "Der Trainings- und der Test-Datensatz werden hier eingelesen. Als Faustregel gilt, der Trainingsdatensatz sollte 80% und der Test-Datensatz 20% des Volumens ausmachen. \n",
    "Der Trainings-Datensatz wird für das trainieren / fitten des Modells verwendet. Der Test-Datensatz beinhaltet eine (in diesem Fall manuell erstellte) sogenannte \"Ground Truth\". Dies ist der Goldstandard, anhand dessen das Modell auf Performance hin überprüft wird. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training-Datensatz\n",
    "training_set = pd.read_csv(\"data/informatikkurse.csv\")  # Liest die CSV-Datei ein und speichert sie in einem DataFrame\n",
    "# training_set = training_set.sample(n=500, random_state=42)  # Zieht eine Zufallsstichprobe von 500 Zeilen aus dem DataFrame mit festgelegtem Seed für Reproduzierbarkeit\n",
    "training_set = training_set.apply(lambda x: x.fillna('') if x.dtype == 'O' else x)  # Ersetzt fehlende Werte durch leere Strings in Objektspalten (Strings) und belässt numerische Spalten unverändert\n",
    "training_set['titel_kursbesch'] = training_set['veranstaltung_titel'] + ' ' + training_set['kursbeschreibung']  # Kombiniert die Spalten \"titel\" und \"kursbeschreibung\" zu einer neuen Spalte \"titel_kursbesch\"\n",
    "docs = training_set['titel_kursbesch'].tolist()  # Konvertiert die Inhalte der Spalte \"titel_kursbesch\" in eine Liste von Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10289785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-Datensatz\n",
    "test_data = pd.read_csv(\"data/bentley_labelled_data_edit.csv\")\n",
    "test_data = test_data.apply(lambda x: x.fillna('') if x.dtype == 'O' else x)  # Ersetzt fehlende Werte durch leere Strings in Objektspalten (Strings) und belässt numerische Spalten unverändert\n",
    "test_data['titel_kursbesch'] = test_data['veranstaltung_titel'] + ' ' + test_data['kursbeschreibung']  # Kombiniert die Spalten \"titel\" und \"kursbeschreibung\" zu einer neuen Spalte \"titel_kursbesch\"\n",
    "ground_truth = test_data[\"Label_ChatGPT\"]  # Spalte im CSV mit manuell erstellten Begriffen, welche man als korrekt erachtet (i.d.R. einfach wichtige Wörter aus dem Text rauskopieren)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb54a56",
   "metadata": {},
   "source": [
    "## NLP Vorbereitungsschritte\n",
    "Zunächst werden die Trainingsdaten eingelesen und die gängigen Vorbereitungsschritte für NLP durchgeführt. Diese wären:\n",
    "* Stopwords entfernen\n",
    "* CountVectorizer spezifizieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58fd3e1",
   "metadata": {},
   "source": [
    "### Stopwords entfernen\n",
    "Im Kontext des hier zu modellierenden Topic Modells werden sowohl standardisierte englische, deutsche als auch individuelle Stopwords generiert und im Objekt `sw` zusammengespielt.\n",
    "Die Stopwords können je nach Anwendungsfall ergänzt oder reduziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a2c5511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vl',\n",
       " 'übung',\n",
       " 'übungen',\n",
       " 'seminar',\n",
       " 'arbeitsgruppenseminar',\n",
       " 'oberseminar',\n",
       " 'proseminar',\n",
       " 'blockveranstaltung',\n",
       " 'vorlesung',\n",
       " 'kolloquium',\n",
       " 'theoriekolloquium',\n",
       " 'einführung',\n",
       " 'tutorium',\n",
       " 'ue',\n",
       " 'vereinbarung',\n",
       " 'projekt',\n",
       " 'praktikum',\n",
       " 'masterprojekt',\n",
       " 'wiederholerklausur',\n",
       " 'fortgeschrittenenpraktikum',\n",
       " 'hauptseminar',\n",
       " 'fachpraktikum',\n",
       " 'ergänzungsvorlesung',\n",
       " 'forschungspraktikum',\n",
       " 'begleitseminar',\n",
       " 'abschlussarbeiten',\n",
       " 'unterrichtspraktikum',\n",
       " 'masterseminar',\n",
       " 'proseminare',\n",
       " 'praxisseminar',\n",
       " 'praxissemester',\n",
       " 'schulpraxis',\n",
       " 'ringpraktikum',\n",
       " 'basispraktikum',\n",
       " 'praxistage',\n",
       " 'industriepraktikum',\n",
       " 'vorkurs',\n",
       " 'projektseminar',\n",
       " 'juniorprofessur',\n",
       " 'masterarbeiten',\n",
       " 'forschungsseminar',\n",
       " 'modulbeschreibung',\n",
       " 'veranstaltung',\n",
       " 'kommentare',\n",
       " 'raum',\n",
       " 'uhrzeit',\n",
       " 'vereinbarung',\n",
       " 'vorlesung',\n",
       " 'praktikum',\n",
       " 'masterprojekt',\n",
       " 'wiederholerklausur',\n",
       " 'fortgeschrittenenpraktikum',\n",
       " 'hauptseminar',\n",
       " 'fachpraktikum',\n",
       " 'ergänzungsvorlesung',\n",
       " 'forschungspraktikum',\n",
       " 'begleitseminar',\n",
       " 'vertiefungsvorlesung',\n",
       " 'projektgruppe',\n",
       " 'anwendungsfach',\n",
       " 'prüfungsleistung',\n",
       " 'studienleistung',\n",
       " 'präsenzstudium',\n",
       " 'selbststudium',\n",
       " 'lehreinheit',\n",
       " 'modulprüfung',\n",
       " 'teilnahmeschein',\n",
       " 'abschlussbericht',\n",
       " 'zwischenbericht',\n",
       " 'beachten',\n",
       " 'wochen',\n",
       " 'endet',\n",
       " 'oberseminar',\n",
       " 'proseminar',\n",
       " 'blockveranstaltung',\n",
       " 'vorlesung',\n",
       " 'kolloquium',\n",
       " 'theoriekolloquium',\n",
       " 'einführung',\n",
       " 'hauptseminar',\n",
       " 'projekteinheit',\n",
       " 'übung',\n",
       " 'tutorium',\n",
       " 'praktikum',\n",
       " 'vertiefung',\n",
       " 'grundlagen',\n",
       " 'workshop',\n",
       " 'vortragsreihe',\n",
       " 'anmeldung',\n",
       " 'abmeldung',\n",
       " 'registrierung',\n",
       " 'einschreibung',\n",
       " 'frist',\n",
       " 'deadline',\n",
       " 'termin',\n",
       " 'zeitraum',\n",
       " 'stichtag',\n",
       " 'prüfungsamt',\n",
       " 'sekretariat',\n",
       " 'verwaltung',\n",
       " 'fakultät',\n",
       " 'zentral',\n",
       " 'raum',\n",
       " 'hörsaal',\n",
       " 'gebäude',\n",
       " 'belegung',\n",
       " 'blockage',\n",
       " 'ausfall',\n",
       " 'verschiebung',\n",
       " 'ects',\n",
       " 'credits',\n",
       " 'leistungspunkte',\n",
       " 'modul',\n",
       " 'modulhandbuch',\n",
       " 'curriculum',\n",
       " 'bachelor',\n",
       " 'master',\n",
       " 'studiengang',\n",
       " 'prüfung',\n",
       " 'klausur',\n",
       " 'hausarbeit',\n",
       " 'referat',\n",
       " 'benotung',\n",
       " 'bewertung',\n",
       " 'schein',\n",
       " 'studium',\n",
       " 'liberale',\n",
       " 'generale',\n",
       " 'wahlbereich',\n",
       " 'e3',\n",
       " 'gasthörer',\n",
       " 'interdisziplinär',\n",
       " 'begleitend',\n",
       " 'übung',\n",
       " 'beispiel',\n",
       " 'studierende',\n",
       " 'grundlage',\n",
       " 'laborübung',\n",
       " 'entwicklung',\n",
       " 'funktion',\n",
       " 'praktikum',\n",
       " 'versuch',\n",
       " 'methode',\n",
       " 'modell',\n",
       " 'kompetente',\n",
       " 'teil',\n",
       " 'ziel',\n",
       " 'schulungsreihe',\n",
       " 'thema',\n",
       " 'teilnehmer',\n",
       " 'bereich',\n",
       " 'präsentation',\n",
       " 'einführung',\n",
       " 'uhr',\n",
       " 'termin',\n",
       " 'überblick',\n",
       " 'aspekt',\n",
       " 'kontakt',\n",
       " 'anmeldung',\n",
       " 'inhalt',\n",
       " 'kontakt',\n",
       " 'sprechstunde',\n",
       " 'anforderung',\n",
       " 'modell',\n",
       " 'aufbau',\n",
       " 'fahren',\n",
       " 'student',\n",
       " 'jeweils',\n",
       " 'studierenden',\n",
       " 'prof',\n",
       " 'seminar',\n",
       " 'seminare',\n",
       " '–',\n",
       " 'fortlaufendes',\n",
       " 'schwerpunktmodul',\n",
       " 'entwickelt',\n",
       " 'handelns',\n",
       " 'ansatz',\n",
       " 'schwerpunktmoduls',\n",
       " 'einzubringen',\n",
       " 'semestern',\n",
       " 'semesternseminar',\n",
       " 'seminarnseminar',\n",
       " 'zulassung',\n",
       " 'rahmen',\n",
       " 'blick',\n",
       " 'bereit',\n",
       " 'teilnehmenden',\n",
       " 'gruppe',\n",
       " 'teilnehmerinnen',\n",
       " 'modul',\n",
       " 'ausdruck',\n",
       " 'einfluß',\n",
       " 'kolloquium',\n",
       " 'thematisiert',\n",
       " 'bereiche',\n",
       " 'vermittelt',\n",
       " 'anhand',\n",
       " 'schwerpunkt',\n",
       " 'seminars',\n",
       " 'lehrperson',\n",
       " 'deutsche',\n",
       " 'form',\n",
       " 'gruppen',\n",
       " 'gegenstand',\n",
       " 'anliegen',\n",
       " 'ansätzen',\n",
       " 'vorgestellt',\n",
       " 'ndie',\n",
       " 'forschungscolleges',\n",
       " 'erprobt',\n",
       " 'denkens',\n",
       " 'ansatzes',\n",
       " 'vorausgesetzt',\n",
       " 'montag',\n",
       " 'studium',\n",
       " 'blockseminar',\n",
       " 'veranstaltung',\n",
       " 'masterstudierende',\n",
       " 'datum',\n",
       " 'immatrikulierte',\n",
       " 'dienstagskolloquium',\n",
       " 'leistungsnachweise',\n",
       " 'leistungsnachweis',\n",
       " 'vgl',\n",
       " 'the',\n",
       " 'studiengangs',\n",
       " 'veranstaltung',\n",
       " 'seminar',\n",
       " 'vorlesung',\n",
       " 'kurs',\n",
       " 'modul',\n",
       " 'angebot',\n",
       " 'behandlung',\n",
       " 'themen',\n",
       " 'einführung',\n",
       " 'grundlagen',\n",
       " 'aufbau',\n",
       " 'überblick',\n",
       " 'inhalt',\n",
       " 'praxis',\n",
       " 'theorie',\n",
       " 'schwerpunkt',\n",
       " 'recht',\n",
       " 'rechts',\n",
       " 'rechtsgebiete',\n",
       " 'rechtswissenschaft',\n",
       " 'rechtsfragen',\n",
       " 'bereich',\n",
       " 'gebiete',\n",
       " 'gesetz',\n",
       " 'juristisch',\n",
       " 'aktuell',\n",
       " 'neue',\n",
       " 'verschiedenen',\n",
       " 'interdisziplinär',\n",
       " 'relevant',\n",
       " 'bedeutung',\n",
       " 'besonders',\n",
       " 'werden',\n",
       " 'sollen',\n",
       " 'kann',\n",
       " 'bietet',\n",
       " 'z.b.',\n",
       " 'u.a.',\n",
       " 'etc.',\n",
       " 'bspw.',\n",
       " 'ggf.',\n",
       " 'i.d.r.',\n",
       " 'family',\n",
       " 'relationship',\n",
       " 'practice',\n",
       " 'lesson',\n",
       " 'better',\n",
       " 'lawsuits',\n",
       " 'they',\n",
       " 'aid',\n",
       " 'united',\n",
       " 'register',\n",
       " 'between',\n",
       " 'language',\n",
       " 'about',\n",
       " 'einfache',\n",
       " 'right',\n",
       " 'acp',\n",
       " 'before',\n",
       " 'common',\n",
       " 'please',\n",
       " 'matrikel',\n",
       " 'registration',\n",
       " 'jurisdictions',\n",
       " 'across',\n",
       " 'limits',\n",
       " 'bundesverfassungsgerichts',\n",
       " 'künftigen',\n",
       " 'naturrecht',\n",
       " 'recent',\n",
       " 'schriftliches',\n",
       " 'literaturverarbeitung',\n",
       " 'kurzvortrag',\n",
       " 'genügt',\n",
       " 'seelmann',\n",
       " 'referats',\n",
       " 'lage',\n",
       " 'fighting',\n",
       " 'neuster',\n",
       " 'arbeitsmethoden',\n",
       " 'eugh',\n",
       " 'algorithmus',\n",
       " 'formel',\n",
       " 'adresse',\n",
       " 'resolution',\n",
       " 'dispute',\n",
       " 'module',\n",
       " 'literaturhinweise',\n",
       " 'civil',\n",
       " 'regulatory',\n",
       " 'menschenwürde',\n",
       " 'machthabern',\n",
       " 'ab',\n",
       " 'aber',\n",
       " 'abermaliges',\n",
       " 'abermals',\n",
       " 'abgerufen',\n",
       " 'abgerufene',\n",
       " 'abgerufener',\n",
       " 'abgerufenes',\n",
       " 'abgesehen',\n",
       " 'acht',\n",
       " 'aehnlich',\n",
       " 'aehnliche',\n",
       " 'aehnlichem',\n",
       " 'aehnlichen',\n",
       " 'aehnlicher',\n",
       " 'aehnliches',\n",
       " 'aehnlichste',\n",
       " 'aehnlichstem',\n",
       " 'aehnlichsten',\n",
       " 'aehnlichster',\n",
       " 'aehnlichstes',\n",
       " 'aeusserst',\n",
       " 'aeusserste',\n",
       " 'aeusserstem',\n",
       " 'aeussersten',\n",
       " 'aeusserster',\n",
       " 'aeusserstes',\n",
       " 'ähnlich',\n",
       " 'ähnliche',\n",
       " 'ähnlichem',\n",
       " 'ähnlichen',\n",
       " 'ähnlicher',\n",
       " 'ähnliches',\n",
       " 'ähnlichst',\n",
       " 'ähnlichste',\n",
       " 'ähnlichstem',\n",
       " 'ähnlichsten',\n",
       " 'ähnlichster',\n",
       " 'ähnlichstes',\n",
       " 'alle',\n",
       " 'allein',\n",
       " 'alleine',\n",
       " 'allem',\n",
       " 'allemal',\n",
       " 'allen',\n",
       " 'allenfalls',\n",
       " 'allenthalben',\n",
       " 'aller',\n",
       " 'allerdings',\n",
       " 'allerlei',\n",
       " 'alles',\n",
       " 'allesamt',\n",
       " 'allg',\n",
       " 'allg.',\n",
       " 'allgemein',\n",
       " 'allgemeine',\n",
       " 'allgemeinem',\n",
       " 'allgemeinen',\n",
       " 'allgemeiner',\n",
       " 'allgemeines',\n",
       " 'allgemeinste',\n",
       " 'allgemeinstem',\n",
       " 'allgemeinsten',\n",
       " 'allgemeinster',\n",
       " 'allgemeinstes',\n",
       " 'allmählich',\n",
       " 'allzeit',\n",
       " 'allzu',\n",
       " 'als',\n",
       " 'alsbald',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'andauernd',\n",
       " 'andauernde',\n",
       " 'andauerndem',\n",
       " 'andauernden',\n",
       " 'andauernder',\n",
       " 'andauerndes',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderenfalls',\n",
       " 'anderer',\n",
       " 'andererseits',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'andernfalls',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'anderst',\n",
       " 'anderweitig',\n",
       " 'anderweitige',\n",
       " 'anderweitigem',\n",
       " 'anderweitigen',\n",
       " 'anderweitiger',\n",
       " 'anderweitiges',\n",
       " 'anerkannt',\n",
       " 'anerkannte',\n",
       " 'anerkannter',\n",
       " 'anerkanntes',\n",
       " 'anfangen',\n",
       " 'anfing',\n",
       " 'angefangen',\n",
       " 'angesetze',\n",
       " 'angesetzt',\n",
       " 'angesetzten',\n",
       " 'angesetzter',\n",
       " 'ans',\n",
       " 'anscheinend',\n",
       " 'ansetzen',\n",
       " 'ansonst',\n",
       " 'ansonsten',\n",
       " 'anstatt',\n",
       " 'anstelle',\n",
       " 'arbeiten',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aufgehört',\n",
       " 'aufgrund',\n",
       " 'aufhören',\n",
       " 'aufhörte',\n",
       " 'aufzusuchen',\n",
       " 'augenscheinlich',\n",
       " 'augenscheinliche',\n",
       " 'augenscheinlichem',\n",
       " 'augenscheinlichen',\n",
       " 'augenscheinlicher',\n",
       " 'augenscheinliches',\n",
       " 'augenscheinlichst',\n",
       " 'augenscheinlichste',\n",
       " 'augenscheinlichstem',\n",
       " 'augenscheinlichsten',\n",
       " 'augenscheinlichster',\n",
       " 'augenscheinlichstes',\n",
       " 'aus',\n",
       " 'ausdrücken',\n",
       " 'ausdrücklich',\n",
       " 'ausdrückliche',\n",
       " 'ausdrücklichem',\n",
       " 'ausdrücklichen',\n",
       " 'ausdrücklicher',\n",
       " 'ausdrückliches',\n",
       " 'ausdrückt',\n",
       " 'ausdrückte',\n",
       " 'ausgenommen',\n",
       " 'ausgenommene',\n",
       " 'ausgenommenem',\n",
       " 'ausgenommenen',\n",
       " 'ausgenommener',\n",
       " 'ausgenommenes',\n",
       " 'ausgerechnet',\n",
       " 'ausgerechnete',\n",
       " 'ausgerechnetem',\n",
       " 'ausgerechneten',\n",
       " 'ausgerechneter',\n",
       " 'ausgerechnetes',\n",
       " 'ausnahmslos',\n",
       " 'ausnahmslose',\n",
       " 'ausnahmslosem',\n",
       " 'ausnahmslosen',\n",
       " 'ausnahmsloser',\n",
       " 'ausnahmsloses',\n",
       " 'außen',\n",
       " 'ausser',\n",
       " 'ausserdem',\n",
       " 'außerhalb',\n",
       " 'äusserst',\n",
       " 'äusserste',\n",
       " 'äusserstem',\n",
       " 'äussersten',\n",
       " 'äusserster',\n",
       " 'äusserstes',\n",
       " 'author',\n",
       " 'autor',\n",
       " 'baelde',\n",
       " 'bald',\n",
       " 'bälde',\n",
       " 'bearbeite',\n",
       " 'bearbeiten',\n",
       " 'bearbeitete',\n",
       " 'bearbeiteten',\n",
       " 'bedarf',\n",
       " 'bedürfen',\n",
       " 'bedurfte',\n",
       " 'been',\n",
       " 'befahl',\n",
       " 'befiehlt',\n",
       " 'befiehlte',\n",
       " 'befohlene',\n",
       " 'befohlens',\n",
       " 'befragen',\n",
       " 'befragte',\n",
       " 'befragten',\n",
       " 'befragter',\n",
       " 'begann',\n",
       " 'beginnen',\n",
       " 'begonnen',\n",
       " 'behalten',\n",
       " 'behielt',\n",
       " 'bei',\n",
       " 'beide',\n",
       " 'beidem',\n",
       " 'beiden',\n",
       " 'beider',\n",
       " 'beiderlei',\n",
       " 'beides',\n",
       " 'beim',\n",
       " 'beinahe',\n",
       " 'beisammen',\n",
       " 'beispielsweise',\n",
       " 'beitragen',\n",
       " 'beitrugen',\n",
       " 'bekannt',\n",
       " 'bekannte',\n",
       " 'bekannter',\n",
       " 'bekanntlich',\n",
       " 'bekanntliche',\n",
       " 'bekanntlichem',\n",
       " 'bekanntlichen',\n",
       " 'bekanntlicher',\n",
       " 'bekanntliches',\n",
       " 'bekennen',\n",
       " 'benutzt',\n",
       " 'bereits',\n",
       " 'berichten',\n",
       " 'berichtet',\n",
       " 'berichtete',\n",
       " 'berichteten',\n",
       " 'besonders',\n",
       " 'besser',\n",
       " 'bessere',\n",
       " 'besserem',\n",
       " 'besseren',\n",
       " 'besserer',\n",
       " 'besseres',\n",
       " 'bestehen',\n",
       " 'besteht',\n",
       " 'bestenfalls',\n",
       " 'bestimmt',\n",
       " 'bestimmte',\n",
       " 'bestimmtem',\n",
       " 'bestimmten',\n",
       " 'bestimmter',\n",
       " 'bestimmtes',\n",
       " 'beträchtlich',\n",
       " 'beträchtliche',\n",
       " 'beträchtlichem',\n",
       " 'beträchtlichen',\n",
       " 'beträchtlicher',\n",
       " 'beträchtliches',\n",
       " 'betraechtlich',\n",
       " 'betraechtliche',\n",
       " 'betraechtlichem',\n",
       " 'betraechtlichen',\n",
       " 'betraechtlicher',\n",
       " 'betraechtliches',\n",
       " 'betreffend',\n",
       " 'betreffende',\n",
       " 'betreffendem',\n",
       " 'betreffenden',\n",
       " 'betreffender',\n",
       " 'betreffendes',\n",
       " 'bevor',\n",
       " 'bez',\n",
       " 'bez.',\n",
       " 'bezgl',\n",
       " 'bezgl.',\n",
       " 'bezueglich',\n",
       " 'bezüglich',\n",
       " 'bietet',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bisher',\n",
       " 'bisherige',\n",
       " 'bisherigem',\n",
       " 'bisherigen',\n",
       " 'bisheriger',\n",
       " 'bisheriges',\n",
       " 'bislang',\n",
       " 'bisschen',\n",
       " 'bist',\n",
       " 'bitte',\n",
       " 'bleiben',\n",
       " 'bleibt',\n",
       " 'blieb',\n",
       " 'bloss',\n",
       " 'böden',\n",
       " 'boeden',\n",
       " 'brachte',\n",
       " 'brachten',\n",
       " 'brauchen',\n",
       " 'braucht',\n",
       " 'bräuchte',\n",
       " 'bringen',\n",
       " 'bsp',\n",
       " 'bsp.',\n",
       " 'bspw',\n",
       " 'bspw.',\n",
       " 'bzw',\n",
       " 'bzw.',\n",
       " 'ca',\n",
       " 'ca.',\n",
       " 'circa',\n",
       " 'da',\n",
       " 'dabei',\n",
       " 'dadurch',\n",
       " 'dafuer',\n",
       " 'dafür',\n",
       " 'dagegen',\n",
       " 'daher',\n",
       " 'dahin',\n",
       " 'dahingehend',\n",
       " 'dahingehende',\n",
       " 'dahingehendem',\n",
       " 'dahingehenden',\n",
       " 'dahingehender',\n",
       " 'dahingehendes',\n",
       " 'dahinter',\n",
       " 'damalige',\n",
       " 'damaligem',\n",
       " 'damaligen',\n",
       " 'damaliger',\n",
       " 'damaliges',\n",
       " 'damals',\n",
       " 'damit',\n",
       " 'danach',\n",
       " 'daneben',\n",
       " 'dank',\n",
       " 'danke',\n",
       " 'danken',\n",
       " 'dann',\n",
       " 'dannen',\n",
       " 'daran',\n",
       " 'darauf',\n",
       " 'daraus',\n",
       " 'darf',\n",
       " 'darfst',\n",
       " 'darin',\n",
       " 'darüber',\n",
       " 'darüberhinaus',\n",
       " 'darueber',\n",
       " 'darueberhinaus',\n",
       " 'darum',\n",
       " 'darunter',\n",
       " 'das',\n",
       " 'daß',\n",
       " 'dass',\n",
       " 'dasselbe',\n",
       " 'Dat',\n",
       " 'davon',\n",
       " 'davor',\n",
       " 'dazu',\n",
       " 'dazwischen',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'dem',\n",
       " 'demgegenüber',\n",
       " 'demgegenueber',\n",
       " 'demgemaess',\n",
       " 'demgemäss',\n",
       " 'demnach',\n",
       " 'demselben',\n",
       " 'den',\n",
       " 'denen',\n",
       " 'denkbar',\n",
       " 'denkbare',\n",
       " 'denkbarem',\n",
       " 'denkbaren',\n",
       " 'denkbarer',\n",
       " 'denkbares',\n",
       " 'denn',\n",
       " 'dennoch',\n",
       " 'denselben',\n",
       " 'der',\n",
       " 'derart',\n",
       " 'derartig',\n",
       " 'derartige',\n",
       " 'derartigem',\n",
       " 'derartigen',\n",
       " 'derartiger',\n",
       " 'derem',\n",
       " 'deren',\n",
       " 'derer',\n",
       " 'derjenige',\n",
       " 'derjenigen',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'derzeit',\n",
       " 'derzeitig',\n",
       " 'derzeitige',\n",
       " 'derzeitigem',\n",
       " 'derzeitigen',\n",
       " 'derzeitiges',\n",
       " 'des',\n",
       " 'deshalb',\n",
       " 'desselben',\n",
       " 'dessen',\n",
       " 'dessenungeachtet',\n",
       " 'desto',\n",
       " 'desungeachtet',\n",
       " 'deswegen',\n",
       " 'dich',\n",
       " 'die',\n",
       " 'diejenige',\n",
       " 'diejenigen',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'diesseitig',\n",
       " 'diesseitige',\n",
       " 'diesseitigem',\n",
       " 'diesseitigen',\n",
       " 'diesseitiger',\n",
       " 'diesseitiges',\n",
       " 'diesseits',\n",
       " 'dinge',\n",
       " 'dir',\n",
       " 'direkt',\n",
       " 'direkte',\n",
       " 'direkten',\n",
       " 'direkter',\n",
       " 'doch',\n",
       " 'doppelt',\n",
       " 'dort',\n",
       " 'dorther',\n",
       " 'dorthin',\n",
       " 'dran',\n",
       " 'drauf',\n",
       " 'drei',\n",
       " 'dreißig',\n",
       " 'drin',\n",
       " 'dritte',\n",
       " 'drüber',\n",
       " 'drueber',\n",
       " 'drum',\n",
       " 'drunter',\n",
       " 'du',\n",
       " 'duerfte',\n",
       " 'duerften',\n",
       " 'duerftest',\n",
       " 'duerftet',\n",
       " 'dunklen',\n",
       " 'durch',\n",
       " 'durchaus',\n",
       " 'durchweg',\n",
       " 'durchwegs',\n",
       " 'dürfen',\n",
       " 'durfte',\n",
       " 'dürfte',\n",
       " 'durften',\n",
       " 'dürften',\n",
       " 'durftest',\n",
       " 'dürftest',\n",
       " 'durftet',\n",
       " 'dürftet',\n",
       " 'eben',\n",
       " 'ebenfalls',\n",
       " 'ebenso',\n",
       " 'ect',\n",
       " 'ect.',\n",
       " 'ehe',\n",
       " 'eher',\n",
       " 'eheste',\n",
       " 'ehestem',\n",
       " 'ehesten',\n",
       " 'ehester',\n",
       " 'ehestes',\n",
       " 'eigen',\n",
       " 'eigene',\n",
       " 'eigenem',\n",
       " 'eigenen',\n",
       " 'eigener',\n",
       " 'eigenes',\n",
       " 'eigenst',\n",
       " 'eigentlich',\n",
       " 'eigentliche',\n",
       " 'eigentlichem',\n",
       " 'eigentlichen',\n",
       " 'eigentlicher',\n",
       " 'eigentliches',\n",
       " 'ein',\n",
       " 'einbaün',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'einerlei',\n",
       " 'einerseits',\n",
       " 'eines',\n",
       " 'einfach',\n",
       " 'einführen',\n",
       " 'einführte',\n",
       " 'einführten',\n",
       " 'eingesetzt',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einigermaßen',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'einmalig',\n",
       " 'einmalige',\n",
       " 'einmaligem',\n",
       " 'einmaligen',\n",
       " 'einmaliger',\n",
       " 'einmaliges',\n",
       " 'eins',\n",
       " 'einseitig',\n",
       " 'einseitige',\n",
       " 'einseitigen',\n",
       " 'einseitiger',\n",
       " 'einst',\n",
       " 'einstmals',\n",
       " 'einzig',\n",
       " 'empfunden',\n",
       " 'ende',\n",
       " 'entgegen',\n",
       " 'entlang',\n",
       " 'entsprechend',\n",
       " 'entsprechende',\n",
       " 'entsprechendem',\n",
       " 'entsprechenden',\n",
       " 'entsprechender',\n",
       " 'entsprechendes',\n",
       " 'entweder',\n",
       " 'er',\n",
       " 'ergänze',\n",
       " 'ergänzen',\n",
       " 'ergänzte',\n",
       " 'ergänzten',\n",
       " 'ergo',\n",
       " 'erhält',\n",
       " 'erhalten',\n",
       " 'erhielt',\n",
       " 'erhielten',\n",
       " 'erneut',\n",
       " 'eröffne',\n",
       " 'eröffnen',\n",
       " 'eröffnet',\n",
       " 'eröffnete',\n",
       " 'eröffnetes',\n",
       " 'erscheinen',\n",
       " 'erst',\n",
       " 'erste',\n",
       " 'erstem',\n",
       " 'ersten',\n",
       " 'erster',\n",
       " 'erstere',\n",
       " 'ersterem',\n",
       " 'ersteren',\n",
       " 'ersterer',\n",
       " 'ersteres',\n",
       " 'erstes',\n",
       " 'es',\n",
       " 'etc',\n",
       " 'etc.',\n",
       " 'etliche',\n",
       " 'etlichem',\n",
       " 'etlichen',\n",
       " 'etlicher',\n",
       " 'etliches',\n",
       " 'etwa',\n",
       " 'etwaige',\n",
       " 'etwas',\n",
       " 'euch',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'euretwegen',\n",
       " 'fall',\n",
       " 'falls',\n",
       " 'fand',\n",
       " 'fast',\n",
       " 'ferner',\n",
       " 'fertig',\n",
       " 'finde',\n",
       " 'finden',\n",
       " 'findest',\n",
       " 'findet',\n",
       " 'folgend',\n",
       " 'folgende',\n",
       " 'folgendem',\n",
       " 'folgenden',\n",
       " 'folgender',\n",
       " 'folgendermassen',\n",
       " 'folgendes',\n",
       " 'folglich',\n",
       " 'for',\n",
       " 'fordern',\n",
       " 'fordert',\n",
       " 'forderte',\n",
       " 'forderten',\n",
       " 'fort',\n",
       " 'fortsetzen',\n",
       " 'fortsetzt',\n",
       " 'fortsetzte',\n",
       " 'fortsetzten',\n",
       " 'fragte',\n",
       " 'frau',\n",
       " 'frei',\n",
       " 'freie',\n",
       " 'freier',\n",
       " 'freies',\n",
       " 'fuer',\n",
       " 'fuers',\n",
       " 'fünf',\n",
       " 'für',\n",
       " 'fürs',\n",
       " 'gab',\n",
       " 'gaenzlich',\n",
       " 'gaenzliche',\n",
       " 'gaenzlichem',\n",
       " 'gaenzlichen',\n",
       " 'gaenzlicher',\n",
       " 'gaenzliches',\n",
       " 'gängig',\n",
       " 'gängige',\n",
       " 'gängigen',\n",
       " 'gängiger',\n",
       " 'gängiges',\n",
       " 'ganz',\n",
       " 'ganze',\n",
       " 'ganzem',\n",
       " 'ganzen',\n",
       " 'ganzer',\n",
       " 'ganzes',\n",
       " 'gänzlich',\n",
       " 'gänzliche',\n",
       " 'gänzlichem',\n",
       " 'gänzlichen',\n",
       " 'gänzlicher',\n",
       " 'gänzliches',\n",
       " 'gar',\n",
       " 'gbr',\n",
       " 'geb',\n",
       " 'geben',\n",
       " 'geblieben',\n",
       " 'gebracht',\n",
       " 'gedurft',\n",
       " 'geehrt',\n",
       " 'geehrte',\n",
       " 'geehrten',\n",
       " 'geehrter',\n",
       " 'gefallen',\n",
       " 'gefälligst',\n",
       " 'gefällt',\n",
       " 'gefiel',\n",
       " 'gegeben',\n",
       " 'gegen',\n",
       " 'gegenüber',\n",
       " 'gegenueber',\n",
       " 'gehabt',\n",
       " 'gehalten',\n",
       " 'gehen',\n",
       " 'geht',\n",
       " 'gekommen',\n",
       " 'gekonnt',\n",
       " 'gemacht',\n",
       " 'gemaess',\n",
       " 'gemäss',\n",
       " 'gemeinhin',\n",
       " 'gemocht',\n",
       " 'genau',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import stopwords_config\n",
    "\n",
    "irrelevant_terms = stopwords_config.irrelevant_terms\n",
    "\n",
    "sw = list(stopwords.get_stopwords(\"en\"))\n",
    "sw.extend(list(stopwords.get_stopwords(\"de\")))\n",
    "sw.extend(irrelevant_terms)\n",
    "irrelevant_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d8f99",
   "metadata": {},
   "source": [
    "### Lemmatisierung\n",
    "Durch Lemmatisierung werden die Wörter in einheitliche Begriffe umgewandelt, sodass diese robuster werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60732494",
   "metadata": {},
   "source": [
    "## Nur zum Nachlesen: Konfiguration, Training und Evaluation des Topic Models\n",
    "Im Folgenden werden die einzelnen Schritte erläutert, wie man automatisiert ein, gemäss dem vordefinierten Goldstandard, möglichst performantes Modell erstellen kann. \n",
    "Dies dient lediglich als Nachschlagewerk, die Code-Zellen daher bitte __NICHT__ ausführen, während man das Topic Model automatisiert erstellen lassen will.\n",
    "Die Anwendung für das aktuelle Topic Modeling wird im nächsten Block durchgeführt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4167a6",
   "metadata": {},
   "source": [
    "### Konfiguration des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f45105",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "Der `CountVectorizer` ist ein wichtiges Werkzeug bei der Vorbereitung von Textdaten für ein Topic Model wie BERTopic. Ein Topic Model analysiert große Mengen an Text, um wiederkehrende Themen (Topics) zu erkennen. BERTopic kombiniert dabei Techniken aus der Natural Language Processing (NLP) mit Clustering-Methoden, um diese Themen zu extrahieren. Der `CountVectorizer` hilft dabei, den Text in eine numerische Darstellung umzuwandeln, die für das Modell nutzbar ist.\n",
    "\n",
    "- `stop_words=sw`:  \n",
    "  Stopwörter (z. B. \"und\", \"der\", \"ein\"), die keine inhaltliche Bedeutung tragen, werden entfernt. Dies stellt sicher, dass das Modell nur auf relevante Begriffe fokussiert ist und keine irrelevanten Wörter in die Themenbildung einfließen.\n",
    "\n",
    "- `token_pattern=r'\\b\\w+\\b'`:  \n",
    "  Der reguläre Ausdruck sorgt dafür, dass nur ganze Wörter als Token betrachtet werden. Sonderzeichen oder isolierte Zahlen werden ausgeschlossen, da sie selten zur inhaltlichen Bedeutung beitragen.\n",
    "\n",
    "- `ngram_range=(1, 3)`:  \n",
    "  Es werden nicht nur einzelne Wörter (1-Gramme), sondern auch Wortkombinationen aus zwei oder drei aufeinanderfolgenden Wörtern (n-Gramme) berücksichtigt. Diese Phrasen wie \"künstliche Intelligenz\" oder \"Datenanalyse\" helfen BERTopic, kontextbezogene und aussagekräftige Themen zu identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f166527",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    stop_words=irrelevant_terms,  # Entfernt Stopwörter basierend auf der angegebenen Liste (sw)\n",
    "    token_pattern=r'\\b\\w+\\b',  # Extrahiert nur ganze Wörter, d. h. keine Sonderzeichen oder Zahlen\n",
    "    ngram_range=(1, 3)  # Erstellt 1-Gramme (einzelne Wörter) bis 3-Gramme (Wortgruppen aus bis zu 3 aufeinanderfolgenden Wörtern)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68b166",
   "metadata": {},
   "source": [
    "#### EmbeddingSettings\n",
    "\n",
    "Die `EmbeddingSettings` definieren die Konfiguration für die Generierung von Text-Embeddings, die zur numerischen Darstellung von Textdaten verwendet werden. Diese Embeddings fassen semantische Ähnlichkeiten und Bedeutungen von Texten in einem Vektorraum zusammen und dienen als Grundlage für weitere Analysen, z. B. Clustering oder Themenmodellierung.\n",
    "\n",
    "### Parameter der EmbeddingSettings\n",
    "\n",
    "- `embedding_model`:  \n",
    "  Gibt das Modell an, das zur Generierung der Embeddings verwendet wird. Hier wird die Klasse `SentenceTransformer` genutzt, die leistungsstarke vortrainierte Transformer-Modelle für die Textverarbeitung unterstützt.\n",
    "\n",
    "- `model_name_or_path`:  \n",
    "  Gibt den Pfad oder Namen des vortrainierten Modells an. In diesem Fall wird das Modell `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` verwendet, das speziell für mehrsprachige Anwendungen optimiert ist. Dieses Modell erzeugt kompakte und semantisch aussagekräftige Embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b634f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbeddingSettings = {\n",
    "    \"embedding_model\": SentenceTransformer,\n",
    "    \"model_name_or_path\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1b815",
   "metadata": {},
   "source": [
    "#### UMAPSettings\n",
    "\n",
    "UMAP (Uniform Manifold Approximation and Projection) ist eine Technik zur Dimensionsreduktion, die häufig in der Verarbeitung von hochdimensionalen Daten, wie Text- oder Bilddaten, eingesetzt wird. Sie projiziert mehrdimensionale Daten in einen Raum mit geringerer Dimension, um Muster und Strukturen leichter zu erkennen. Die `UMAPSettings` definieren die Parameter, die das Verhalten und die Genauigkeit dieser Projektion steuern.\n",
    "\n",
    "\n",
    "- `n_neighbors=15`:  \n",
    "  Gibt die Anzahl der nächsten Nachbarn an, die für jeden Punkt berücksichtigt werden. Ein höherer Wert fokussiert auf größere Strukturen in den Daten, während ein niedrigerer Wert stärker lokale Muster betont.\n",
    "\n",
    "- `n_components=5`:  \n",
    "  Legt die Dimension des reduzierten Raumes fest. In diesem Fall werden die Daten in 5 Dimensionen projiziert, was hilft, wesentliche Eigenschaften der Daten zu erhalten.\n",
    "\n",
    "- `min_dist=0.1`:  \n",
    "  Bestimmt, wie nah Punkte im projizierten Raum beieinander liegen können. Ein niedriger Wert führt zu eng gepackten Clustern, während ein höherer Wert eine gleichmäßigere Verteilung ermöglicht.\n",
    "\n",
    "- `metric=\"cosine\"`:  \n",
    "  Gibt die Distanzmetrik an, die verwendet wird, um die Ähnlichkeit zwischen Punkten zu berechnen. Der Kosinusabstand ist besonders geeignet für Textdaten oder hochdimensionale Vektoren.\n",
    "\n",
    "- `random_state=13`:  \n",
    "  Definiert einen Seed-Wert für den Zufallszahlengenerator, um reproduzierbare Ergebnisse sicherzustellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abbbfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAPSettings = {\n",
    "    \"n_neighbors\": 15,\n",
    "    \"n_components\": 5,\n",
    "    \"min_dist\": 0.1,\n",
    "    \"metric\": \"cosine\",\n",
    "    \"random_state\": 13\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba02d1c",
   "metadata": {},
   "source": [
    "#### HDBSCANSettings\n",
    "\n",
    "HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) ist ein Clustering-Algorithmus, der besonders gut mit komplexen Datensätzen umgehen kann. Er identifiziert Cluster auf der Basis von Dichte und ermöglicht es, Datenpunkte als *Rauschen* zu klassifizieren, wenn sie nicht zu einem Cluster gehören. Die `HDBSCANSettings` definieren die Parameter, die das Verhalten des Algorithmus steuern.\n",
    "\n",
    "##### Parameter der HDBSCANSettings\n",
    "\n",
    "- `min_samples=10`:  \n",
    "  Gibt die minimale Anzahl von Datenpunkten an, die in der Nachbarschaft eines Punktes vorhanden sein müssen, damit er als Kernpunkt eines Clusters gilt. Ein höherer Wert macht den Algorithmus empfindlicher gegenüber Rauschen.\n",
    "\n",
    "- `gen_min_span_tree=True`:  \n",
    "  Erstellt einen minimalen `span tree`, der die hierarchische Struktur der Cluster visualisiert. Dies ist nützlich für die Analyse und Interpretation der Ergebnisse.\n",
    "\n",
    "- `prediction_data=True`:  \n",
    "  Ermöglicht die Generierung zusätzlicher Daten, die für die spätere Zuordnung neuer Punkte zu den Clustern verwendet werden können.\n",
    "\n",
    "- `min_cluster_size=100`:  \n",
    "  Gibt die minimale Größe eines Clusters an. Cluster mit weniger Datenpunkten werden als Rauschen betrachtet und nicht berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffde44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDBSCANSettings = {\n",
    "    \"min_samples\": 3,\n",
    "    \"gen_min_span_tree\": True,\n",
    "    \"prediction_data\": True,\n",
    "    \"min_cluster_size\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1ec40",
   "metadata": {},
   "source": [
    "#### BERTopicSettings\n",
    "\n",
    "Die `BERTopicSettings` definieren die Konfiguration des BERTopic-Modells, das zur Themenmodellierung verwendet wird. BERTopic identifiziert wiederkehrende Themen in Textdaten und ermöglicht eine flexible Anpassung der Ergebnisse durch verschiedene Parameter.\n",
    "\n",
    "#### Parameter der BERTopicSettings\n",
    "\n",
    "- `top_n_words=10`:  \n",
    "  Gibt an, wie viele Schlüsselwörter pro Thema angezeigt werden. Ein höherer Wert liefert detailliertere Informationen zu den Themen.\n",
    "\n",
    "- `language=\"multilingual\"`:  \n",
    "  Setzt die Sprache für die Verarbeitung von Textdaten. Mit \"multilingual\" wird sichergestellt, dass Texte in mehreren Sprachen unterstützt werden.\n",
    "\n",
    "- `n_gram_range=(1, 4)`:  \n",
    "  Bestimmt den Bereich der n-Gramme (z. B. einzelne Wörter bis zu Vier-Wort-Kombinationen), die für die Themenanalyse berücksichtigt werden.\n",
    "\n",
    "- `min_topic_size=100`:  \n",
    "  Legt die minimale Anzahl von Dokumenten fest, die ein Thema enthalten muss, damit es berücksichtigt wird. Kleinere Mindestgrößen ermöglichen es, mehr spezialisierte Themen zu erkennen.\n",
    "\n",
    "- `calculate_probabilities=True`:  \n",
    "  Aktiviert die Berechnung von Wahrscheinlichkeiten, die die Zugehörigkeit von Dokumenten zu bestimmten Themen darstellen.\n",
    "\n",
    "- `verbose=True`:  \n",
    "  Aktiviert detaillierte Konsolenausgaben, die den Fortschritt des Modells anzeigen.\n",
    "\n",
    "- `nr_topics=20`:  \n",
    "  Setzt die Anzahl der finalen Themen auf 20. Diese Begrenzung wird durch die Reduktion ähnlicher Themen erreicht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bc1e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTopicSettings = {\n",
    "    \"top_n_words\": 10,  # Mehr Top-Wörter, um relevantere Themen zu erfassen\n",
    "    \"language\": \"multilingual\",\n",
    "    \"n_gram_range\": (1, 3),  # Erweiterung des n-Gram-Bereichs\n",
    "    \"min_topic_size\": 10,  # Kleinere Mindestgröße der Themen\n",
    "    \"calculate_probabilities\": True,\n",
    "    \"verbose\": True,\n",
    "    \"nr_topics\": None,  # Festlegung der Anzahl der Themen\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d775dfb",
   "metadata": {},
   "source": [
    "#### BERTopic: Initialisieren und trainieren\n",
    "\n",
    "Im folgenden wird das Modell initialisiert.\n",
    "\n",
    "##### Parameter BERTopic\n",
    "\n",
    "- `ctfidf_model`:  \n",
    "  Ein ClassTfidfTransformer wird mit dem Parameter `reduce_frequent_words=True` initialisiert. Dies reduziert den Einfluss hochfrequenter Wörter, die zusätzlich zu Stopwörtern das Modell verzerren könnten.\n",
    "\n",
    "- `topic_model`:  \n",
    "  Initialisierung des BERTopic-Modells mit verschiedenen benutzerdefinierten Einstellungen:\n",
    "  - `ctfidf_model=ctfidf_model`:  \n",
    "    Verwendet das zuvor definierte ClassTfidfTransformer-Modell, um hochfrequente Wörter zu behandeln.\n",
    "  - `vectorizer_model=vectorizer`:  \n",
    "    Übergibt den zuvor definierten CountVectorizer, der für die Tokenisierung und n-Gramm-Erstellung genutzt wird.\n",
    "  - `embedding_model=EmbeddingSettings[\"embedding_model\"](EmbeddingSettings[\"model_name_or_path\"])`:  \n",
    "    Erstellt ein Text-Embedding-Modell basierend auf den in EmbeddingSettings angegebenen Parametern.\n",
    "  - `umap_model=UMAP(**UMAPSettings)`:  \n",
    "    Nutzt UMAP zur Dimensionsreduktion mit den vorher definierten Einstellungen in UMAPSettings.\n",
    "  - `hdbscan_model=HDBSCAN(**HDBSCANSettings)`:  \n",
    "    Führt das Clustering mit HDBSCAN durch, basierend auf den Einstellungen in HDBSCANSettings.\n",
    "  - `**BERTopicSettings`:  \n",
    "    Übernimmt zusätzliche Parameter aus BERTopicSettings, wie die Anzahl der Themen oder die Sprache.\n",
    "\n",
    "- Modelltraining und Transformation:  \n",
    "  Der Datensatz wird durch das `fit_transform`-Verfahren verarbeitet:\n",
    "  - `topics`: Enthält die identifizierten Themen für jedes Dokument.\n",
    "  - `probs`: Liefert die Wahrscheinlichkeiten, mit denen ein Dokument zu einem Thema gehört.\n",
    "\n",
    "- Themenanalyse:  \n",
    "  `topic_model.get_topic_info()` gibt eine Übersicht der erkannten Themen, deren Häufigkeit und Repräsentation zurück. Hier werden die Top 50 Themen extrahiert und nach ihrer ID aufgelistet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fef1cd",
   "metadata": {},
   "source": [
    "#### Definition Objective\n",
    "Hier wird das Package \"optuna\" verwendet, durch welches die zu testenden Parameter festgelegt werden können und anhand einer definierten Metrik optimiert werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782d9b0",
   "metadata": {},
   "source": [
    "## Anwendung: Konfiguration, Training und Evaluation des Topic Models\n",
    "Hier muss alles innerhalb einer einzigen Code-Zelle erfolgen, da bei allen Konfigurationen variable Parameter vorkommen und wir diese durch das optuna-Package optimieren wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im Objective werden die verschiedenen Parameter-Settings gesetzt, über welche man optimieren möchte\n",
    "def objective(trial):\n",
    "\n",
    "  try:\n",
    "    # Embedding Settings\n",
    "    embedding_model_name = trial.suggest_categorical(\"embedding_model\", [\"paraphrase-multilingual-MiniLM-L12-v2\", \"paraphrase-mpnet-base-v2\"])\n",
    "    # UMAP Settings\n",
    "    n_neighbors = trial.suggest_int(\"n_neighbors\", 4, 14, 2)\n",
    "    min_dist = trial.suggest_float(\"min_dist\", 0.0, 0.2, step = 0.1)\n",
    "    n_components = trial.suggest_int(\"n_components\", 3, 8, 1)\n",
    "    # HDBSCAN Settings\n",
    "    min_cluster_size = trial.suggest_int(\"min_cluster_size\", 3, 8, 1)\n",
    "    #min_samples = trial.suggest_int(\"min_samples\", 3, 10, 1)\n",
    "    # BERTopic Settings\n",
    "    #nr_topics = trial.suggest_categorical(\"nr_topics\", [5, 6, 7, 8, 9, 10, 15])\n",
    "    diversity = trial.suggest_float(\"diversity\", 0.0, 0.2, step = 0.1)\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Konfiguration\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # CountVectorizer\n",
    "    vectorizer = CountVectorizer(\n",
    "      stop_words=sw,  # Entfernt Stopwörter basierend auf der angegebenen Liste (sw)\n",
    "      token_pattern=r'\\b\\w+\\b',  # Extrahiert nur ganze Wörter, d. h. keine Sonderzeichen oder Zahlen\n",
    "      ngram_range=(1, 3)  # Erstellt 1-Gramme (einzelne Wörter) bis 3-Gramme (Wortgruppen aus bis zu 3 aufeinanderfolgenden Wörtern)\n",
    "    )\n",
    "\n",
    "    # Embedding Settings  \n",
    "    embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    \n",
    "    # UMAP Settings\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, metric=\"cosine\", random_state=13)\n",
    "\n",
    "    # HDBSCAN Settings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, gen_min_span_tree=True, prediction_data=True)\n",
    "\n",
    "    # Representation Settings\n",
    "    representation_model = MaximalMarginalRelevance(diversity=diversity)\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Training\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # BERTopic initialisieren\n",
    "    topic_model = BERTopic(\n",
    "      embedding_model=embedding_model,\n",
    "      #min_topic_size=10,\n",
    "      #nr_topics=nr_topics, \n",
    "      language=\"multilingual\",\n",
    "      umap_model=umap_model,\n",
    "      vectorizer_model=vectorizer,\n",
    "      hdbscan_model=hdbscan_model,\n",
    "      top_n_words = 30,\n",
    "      representation_model=representation_model\n",
    "    )\n",
    "\n",
    "    # BERTopic trainieren\n",
    "    topic_model_quanten = topic_model.fit(docs)\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Evaluation\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # BERTopic auf Test-Daten anwenden\n",
    "    topics, probs = topic_model_quanten.transform(test_set)\n",
    "    print(topic_model_quanten.get_topic_freq())\n",
    "\n",
    "    # Outlier reduzieren\n",
    "    topics = topic_model_quanten.reduce_outliers(test_set, topics)\n",
    "\n",
    "    # Resultierende Topic-Nummern mit den Representations (= relevante Begriffe) zu einem Datensatz kombinieren\n",
    "    dataframe_with_results_left = pd.DataFrame(topics, columns = [\"Topic\"])\n",
    "    dataframe_with_results_right = pd.DataFrame(topic_model_quanten.get_topic_info().set_index('Topic')[['Representation']])\n",
    "    dataframe_with_results = dataframe_with_results_left.join(dataframe_with_results_right, on=\"Topic\")\n",
    "\n",
    "    # Goldstandard (Ground Truth) mit den Ergebnissen abgleichen und Score berechnen (Score = Anteil korrekter Topic-Zuweisungen)\n",
    "    row_number = 0\n",
    "    metric = 0\n",
    "    while row_number < len(ground_truth):\n",
    "      # Den Goldstandard in eine Liste von Keywords umwandeln\n",
    "      ground_truth_current_iteration = ground_truth[row_number].split(\", \")\n",
    "      result_current_iteration = dataframe_with_results.at[row_number, \"Representation\"]\n",
    "\n",
    "      # Überprüfen, ob irgendein Begriff aus dem Resultat im Goldstandard zum Text vorkommt (1 = ja, 0 = nein)\n",
    "      if any(element in result_current_iteration for element in ground_truth_current_iteration):\n",
    "              metric += 1\n",
    "      else: metric += 0\n",
    "\n",
    "      row_number = row_number+1\n",
    "\n",
    "      print(result_current_iteration)\n",
    "      print(ground_truth_current_iteration)\n",
    "      print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    metric_score = metric/row_number\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Ergebnis printen und Score returnen\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Ergebnis printen\n",
    "    #print(\"Modell evaluiert mit einem Score von \", metric_score, \". \" \\\n",
    "    #\"Verwendete Parameter: embedding model: \", embedding_model_name, \", nr_topics: \", nr_topics, \", n_neighbors: \", n_neighbors, \", min_dist: \", min_dist,\n",
    "    #\", n_components: \", n_components, \", min_cluster_size: \", min_cluster_size)\n",
    "  \n",
    "    return metric_score \n",
    "  \n",
    "  except Exception as e:\n",
    "      print(\"Trial wird aufgrund eines Errors übersprungen\")\n",
    "      print(\"Verwendete Parameter: embedding model: \", embedding_model_name, \", n_neighbors: \", n_neighbors, \", min_dist: \", min_dist,\\\n",
    "      \", n_components: \", n_components, \", min_cluster_size: \", min_cluster_size)\n",
    "      print(e)\n",
    "      raise optuna.TrialPruned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd575faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction = \"maximize\")\n",
    "study.optimize(objective, n_trials = 100)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
