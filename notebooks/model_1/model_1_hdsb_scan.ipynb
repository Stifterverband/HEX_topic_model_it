{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d2dc7e",
   "metadata": {},
   "source": [
    "# Template für Topic Modeling\n",
    "Dieses Template soll dabei helfen, Topic Modeling automatisiert und einheitlich durchzuführen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3a5f4",
   "metadata": {},
   "source": [
    "## Allgemeine Vorbereitungsschritte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634674f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktuelles Arbeitsverzeichnis anzeigen und bei Bedarf anpassen\n",
    "# print(os.getcwd())\n",
    "# os.chdir(\"C:/SV/HEX/Topic Modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d61603",
   "metadata": {},
   "source": [
    "### Pakete laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1471af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktuelles Arbeitsverzeichnis anzeigen und bei Bedarf anpassen\n",
    "import os\n",
    "print(os.getcwd())\n",
    "#os.chdir(\"C:/Users/mhu/Documents/github/topic_model_it/\")\n",
    "#os.chdir(\"C:/Users/mhu/Documents/github/HEX_topic_model_it/\") \n",
    "os.chdir(\"C:/Users/Hueck/OneDrive/Dokumente/GitHub/HEX_topic_model_it\")\n",
    "import pandas as pd\n",
    "import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from bertopic.vectorizers import ClassTfidfTransformer \n",
    "import openpyxl\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "print(f\"NumPy Version: {np.__version__}\") # Sollte 1.26.x sein\n",
    "print(f\"CUDA verfügbar: {torch.cuda.is_available()}\") # Sollte TRUE sein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feeb504",
   "metadata": {},
   "source": [
    "### Seed setzen\n",
    "Wir setzen einen festen Seed, um Zufallszahlen in NumPy und PyTorch reproduzierbar zu machen, sowohl auf der CPU als auch auf der GPU (falls verfügbar). Das stellt sicher, dass Berechnungen mit zufälligen Operationen bei wiederholter Ausführung dieselben Ergebnisse liefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d700e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 40  # Initialisiert den Seed-Wert für reproduzierbare Ergebnisse\n",
    "np.random.seed(seed)  # Setzt den Seed für NumPy-Zufallszahlengeneratoren\n",
    "random.seed(seed)  # Setzt den Seed für den Python-eigenen Zufallszahlengenerator\n",
    "torch.manual_seed(seed)  # Setzt den Seed für PyTorch-Zufallszahlen\n",
    "if torch.cuda.is_available():  # Überprüft, ob CUDA (GPU-Unterstützung) verfügbar ist\n",
    "    torch.cuda.manual_seed_all(seed)  # Setzt den Seed für alle CUDA-Zufallszahlen (für GPU-Berechnungen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99ec26",
   "metadata": {},
   "source": [
    "### Datensätze einlesen\n",
    "Der Trainings- und der Test-Datensatz werden hier eingelesen. Als Faustregel gilt, der Trainingsdatensatz sollte 80% und der Test-Datensatz 20% des Volumens ausmachen. \n",
    "Der Trainings-Datensatz wird für das trainieren / fitten des Modells verwendet. Der Test-Datensatz beinhaltet eine (in diesem Fall manuell erstellte) sogenannte \"Ground Truth\". Dies ist der Goldstandard, anhand dessen das Modell auf Performance hin überprüft wird. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training-Datensatz\n",
    "training_set = pd.read_csv(\"data/informatikkurse.csv\")  # Liest die CSV-Datei ein und speichert sie in einem DataFrame\n",
    "# training_set = training_set.sample(n=500, random_state=42)  # Zieht eine Zufallsstichprobe von 500 Zeilen aus dem DataFrame mit festgelegtem Seed für Reproduzierbarkeit\n",
    "training_set = training_set.apply(lambda x: x.fillna('') if x.dtype == 'O' else x)  # Ersetzt fehlende Werte durch leere Strings in Objektspalten (Strings) und belässt numerische Spalten unverändert\n",
    "training_set['titel_kursbesch'] = training_set['veranstaltung_titel'] + ' ' + training_set['kursbeschreibung']  # Kombiniert die Spalten \"titel\" und \"kursbeschreibung\" zu einer neuen Spalte \"titel_kursbesch\"\n",
    "docs = training_set['titel_kursbesch'].tolist()  # Konvertiert die Inhalte der Spalte \"titel_kursbesch\" in eine Liste von Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d20bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10289785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Daten laden und NaNs bereinigen\n",
    "df = pd.read_csv(\"data/labelled_data_final.csv\", sep=\";\").fillna('')\n",
    "\n",
    "# 2. Kombination und Vorab-Filterung (entfernt leere Synonyme)\n",
    "df['titel_kursbesch'] = df['veranstaltung_titel'] + ' ' + df['kursbeschreibung']\n",
    "df = df[df[\"Synonyme\"].str.contains(r'[a-zA-Z0-9]')] # Behält nur Zeilen mit echten Zeichen (isalnum)\n",
    "\n",
    "# 3. Ground Truth: In einem Rutsch splitten und bereinigen\n",
    "# Wir nutzen .str.split mit Regex, um Whitespace um Kommas direkt zu entfernen\n",
    "ground_truth = (\n",
    "    df[\"Synonyme\"]\n",
    "    .str.lower()\n",
    "    .str.split(r'\\s*,\\s*')\n",
    "    .apply(lambda terms: [t for t in terms if t.strip()]) # Filtert leere Strings innerhalb der Liste\n",
    ").tolist()\n",
    "\n",
    "# 4. Finales Test-Set\n",
    "test_set = df['titel_kursbesch'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb54a56",
   "metadata": {},
   "source": [
    "## NLP Vorbereitungsschritte\n",
    "Zunächst werden die Trainingsdaten eingelesen und die gängigen Vorbereitungsschritte für NLP durchgeführt. Diese wären:\n",
    "* Stopwords entfernen\n",
    "* CountVectorizer spezifizieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58fd3e1",
   "metadata": {},
   "source": [
    "### Stopwords entfernen\n",
    "Im Kontext des hier zu modellierenden Topic Modells werden sowohl standardisierte englische, deutsche als auch individuelle Stopwords generiert und im Objekt `sw` zusammengespielt.\n",
    "Die Stopwords können je nach Anwendungsfall ergänzt oder reduziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import stopwords_config\n",
    "\n",
    "irrelevant_terms = stopwords_config.irrelevant_terms\n",
    "\n",
    "sw = list(stopwords.get_stopwords(\"en\"))\n",
    "sw.extend(list(stopwords.get_stopwords(\"de\")))\n",
    "sw.extend(irrelevant_terms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab8b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d8f99",
   "metadata": {},
   "source": [
    "### Lemmatisierung\n",
    "Durch Lemmatisierung werden die Wörter in einheitliche Begriffe umgewandelt, sodass diese robuster werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782d9b0",
   "metadata": {},
   "source": [
    "## Anwendung: Konfiguration, Training und Evaluation des Topic Models\n",
    "Hier muss alles innerhalb einer einzigen Code-Zelle erfolgen, da bei allen Konfigurationen variable Parameter vorkommen und wir diese durch das optuna-Package optimieren wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9970f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im Objective werden die verschiedenen Parameter-Settings gesetzt, über welche man optimieren möchte\n",
    "def objective(trial):\n",
    "\n",
    "  try:\n",
    "    # Embedding Settings - GPU-beschleunigt\n",
    "    embedding_model_name = trial.suggest_categorical(\"embedding_model\", [\"paraphrase-multilingual-mpnet-base-v2\"])\n",
    "    # UMAP Settings\n",
    "    n_neighbors = trial.suggest_categorical(\"n_neighbors\", [10, 15, 25])\n",
    "    min_dist = 0.0\n",
    "    n_components = trial.suggest_int(\"n_components\", 5, 15, 2)\n",
    "    # HDBSCAN Settings - KLEINERE Cluster für MEHR spezifische Topics\n",
    "    min_cluster_size = trial.suggest_int(\"min_cluster_size\", 5, 20, 5)\n",
    "    min_samples = trial.suggest_int(\"min_samples\", 1, 3, 1)\n",
    "    # BERTopic Settings - angepasst auf ~40 Labels\n",
    "    nr_topics = trial.suggest_int(\"nr_topics\", 35, 50, 5)\n",
    "    diversity = trial.suggest_float(\"diversity\", 0.2, 0.5, step=0.1)\n",
    "    min_topic_size = trial.suggest_int(\"min_topic_size\", 5, 20, 5)\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Konfiguration\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # CountVectorizer\n",
    "    vectorizer = CountVectorizer(\n",
    "      stop_words=sw,\n",
    "      token_pattern=r'\\b\\w+\\b',\n",
    "      ngram_range=(1, 2),\n",
    "      max_features=10000\n",
    "    )\n",
    "\n",
    "    # Embedding Settings - GPU-beschleunigt für RTX 4060\n",
    "    embedding_model = SentenceTransformer(embedding_model_name, device=\"cuda\")\n",
    "    \n",
    "    # UMAP Settings\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, metric=\"cosine\", random_state=13, low_memory=True)\n",
    "\n",
    "    # HDBSCAN Settings\n",
    "    hdbscan_model = HDBSCAN(\n",
    "      min_cluster_size=min_cluster_size, \n",
    "      min_samples=min_samples,\n",
    "      gen_min_span_tree=False, \n",
    "      prediction_data=True\n",
    "    )\n",
    "\n",
    "    # Representation Settings\n",
    "    representation_model = MaximalMarginalRelevance(diversity=diversity)\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Training\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # BERTopic initialisieren\n",
    "    topic_model = BERTopic(\n",
    "      embedding_model=embedding_model,\n",
    "      min_topic_size=min_topic_size,\n",
    "      nr_topics=nr_topics, \n",
    "      language=\"multilingual\",\n",
    "      umap_model=umap_model,\n",
    "      vectorizer_model=vectorizer,\n",
    "      hdbscan_model=hdbscan_model,\n",
    "      top_n_words=30,\n",
    "      representation_model=representation_model,\n",
    "      calculate_probabilities=False,\n",
    "      low_memory=True\n",
    "    )\n",
    "\n",
    "    # BERTopic trainieren\n",
    "    topic_model_quanten = topic_model.fit(docs)\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Topic-Statistik ausgeben\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    topic_info = topic_model_quanten.get_topic_info()\n",
    "    num_topics = len(topic_info[topic_info['Topic'] != -1])  # Ohne Outlier\n",
    "    outlier_count = topic_info[topic_info['Topic'] == -1]['Count'].values[0] if -1 in topic_info['Topic'].values else 0\n",
    "    outlier_ratio = outlier_count / len(docs)\n",
    "    \n",
    "    print(f\"\\n=== TOPIC STATISTIK ===\")\n",
    "    print(f\"Anzahl Topics: {num_topics}\")\n",
    "    print(f\"Outlier: {outlier_count} ({outlier_ratio:.1%})\")\n",
    "    print(f\"========================\\n\")\n",
    "    \n",
    "    # Trial abbrechen wenn zu wenige Topics (mindestens 15 Topics erforderlich)\n",
    "    if num_topics < 15:\n",
    "        print(f\"Nur {num_topics} Topics - Trial wird übersprungen\")\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Evaluation mit Embedding-basierter Similarity\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    # BERTopic auf Test-Daten anwenden\n",
    "    topics, probs = topic_model_quanten.transform(test_set)\n",
    "    print(topic_model_quanten.get_topic_freq())\n",
    "\n",
    "    # Resultierende Topic-Nummern mit den Representations kombinieren\n",
    "    dataframe_with_results_left = pd.DataFrame(topics, columns=[\"Topic\"])\n",
    "    dataframe_with_results_right = pd.DataFrame(topic_model_quanten.get_topic_info().set_index('Topic')[['Representation']])\n",
    "    dataframe_with_results = dataframe_with_results_left.join(dataframe_with_results_right, on=\"Topic\")\n",
    "\n",
    "    # Goldstandard abgleichen mit Embedding-Similarity\n",
    "    similarity_threshold = 0.6  # Schwellenwert für Match (0.6 = moderat, 0.7 = streng)\n",
    "    metric = 0\n",
    "    \n",
    "    for row_number in range(len(ground_truth)):\n",
    "      ground_truth_current_iteration = ground_truth[row_number]\n",
    "      result_current_iteration = dataframe_with_results.at[row_number, \"Representation\"]\n",
    "\n",
    "      # Embeddings berechnen\n",
    "      gt_embeddings = embedding_model.encode(ground_truth_current_iteration)\n",
    "      tm_embeddings = embedding_model.encode(result_current_iteration)\n",
    "      \n",
    "      # Cosine Similarity zwischen allen Paaren\n",
    "      sim_matrix = cosine_similarity(gt_embeddings, tm_embeddings)\n",
    "      max_sim = sim_matrix.max()\n",
    "      \n",
    "      # Match wenn Ähnlichkeit über Schwellenwert\n",
    "      match_found = max_sim >= similarity_threshold\n",
    "      if match_found:\n",
    "        metric += 1\n",
    "\n",
    "      print(\"TM:\", result_current_iteration[:5], \"...\")  # Nur erste 5 Wörter\n",
    "      print(\"Ground Truth:\", ground_truth_current_iteration)\n",
    "      print(f\"Max Similarity: {max_sim:.3f} → {'MATCH ✓' if match_found else 'NO MATCH ✗'}\")\n",
    "      print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    metric_score = metric / len(ground_truth)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Kombinierte Metrik: Score * Topic-Qualität\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Bestraft Modelle mit weniger als 40 Topics (dein Ziel)\n",
    "    topic_penalty = min(num_topics / 40, 1.0)\n",
    "    adjusted_score = metric_score * topic_penalty\n",
    "    \n",
    "    print(f\"\\n=== FINAL ERGEBNIS ===\")\n",
    "    print(f\"Raw Score: {metric_score:.3f} ({metric}/{len(ground_truth)} Matches)\")\n",
    "    print(f\"Topic Penalty: {topic_penalty:.3f} ({num_topics}/40 Topics)\")\n",
    "    print(f\"Adjusted Score: {adjusted_score:.3f}\")\n",
    "    print(f\"======================\\n\")\n",
    "\n",
    "    return adjusted_score \n",
    "  \n",
    "  except Exception as e:\n",
    "      print(\"Trial wird aufgrund eines Errors übersprungen\")\n",
    "      print(f\"Verwendete Parameter: embedding model: {embedding_model_name}, n_neighbors: {n_neighbors}, \"\n",
    "            f\"min_dist: {min_dist}, n_components: {n_components}, min_cluster_size: {min_cluster_size}, \"\n",
    "            f\"min_samples: {min_samples}\")\n",
    "      print(e)\n",
    "      raise optuna.TrialPruned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd575faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Trials anzeigen\n",
    "import pandas as pd\n",
    "\n",
    "# Alle Trials als DataFrame\n",
    "trials_df = study.trials_dataframe()\n",
    "\n",
    "# Nach Score sortieren (absteigend) und Top 10 anzeigen\n",
    "top_10 = trials_df.nlargest(10, 'value')[['number', 'value', 'params_n_neighbors', 'params_n_components', \n",
    "                                           'params_min_cluster_size', 'params_min_samples', \n",
    "                                           'params_nr_topics', 'params_diversity', 'params_min_topic_size']]\n",
    "print(\"=== TOP 10 TRIALS ===\")\n",
    "print(top_10.to_string())\n",
    "\n",
    "# Oder kompakter:\n",
    "print(\"\\n=== TOP 10 BESTE PARAMETER ===\")\n",
    "for i, trial in enumerate(sorted(study.trials, key=lambda t: t.value if t.value else 0, reverse=True)[:10]):\n",
    "    print(f\"{i+1}. Trial {trial.number}: Score={trial.value:.3f}\")\n",
    "    print(f\"   {trial.params}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3913127",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df\n",
    "trials_df.to_csv(\"model_1_hdsb_scan_trials.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"best_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f40bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"best_params.json\", \"r\") as f:\n",
    "    best_params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4cf9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Modell mit best_params neu aufsetzen\n",
    "embedding_model = SentenceTransformer(best_params['embedding_model'], device=\"cuda\")\n",
    "umap_model = UMAP(n_neighbors=best_params['n_neighbors'], min_dist=0.0, n_components=best_params['n_components'], metric=\"cosine\", random_state=13, low_memory=True)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=best_params['min_cluster_size'], min_samples=best_params['min_samples'], gen_min_span_tree=False, prediction_data=True)\n",
    "vectorizer = CountVectorizer(stop_words=sw, token_pattern=r'\\b\\w+\\b', ngram_range=(1, 2), max_features=10000)\n",
    "representation_model = MaximalMarginalRelevance(diversity=best_params['diversity'])\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    min_topic_size=best_params['min_topic_size'],\n",
    "    nr_topics=best_params['nr_topics'],\n",
    "    language=\"multilingual\",\n",
    "    umap_model=umap_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    top_n_words=30,\n",
    "    representation_model=representation_model,\n",
    "    calculate_probabilities=False,\n",
    "    low_memory=True\n",
    ")\n",
    "topic_model_quanten = topic_model.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_quanten.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativ mit pop (verhindert Fehler, falls Key nicht existiert)\n",
    "os.environ.pop(\"OPENAI_API_KEY\", None)\n",
    "\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))  # Ausgabe: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63139f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Lädt die .env-Datei im aktuellen Verzeichnis\n",
    "import os\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))  # Sollte deinen Key anzeigen (oder None, wenn nicht gefunden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "try:\n",
    "    client.models.list()\n",
    "    print(\"Verbindung zu OpenAI erfolgreich!\")\n",
    "except Exception as e:\n",
    "    print(\"Fehler:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd941db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from bertopic.representation import OpenAI as OpenAIRep\n",
    "\n",
    "# 1. Client initialisieren (er findet os.getenv(\"OPENAI_API_KEY\") automatisch)\n",
    "client = OpenAI() \n",
    "\n",
    "# 2. Das Representation Model konfigurieren\n",
    "# WICHTIG: BERTopic erwartet den Client oft im 'client' Parameter \n",
    "# ODER nutzt direkt die Umgebungsvariablen.\n",
    "rep = OpenAIRep(\n",
    "    client=client, \n",
    "    model=\"gpt-4o-mini\", \n",
    "    chat=True,                # Wichtig für gpt-4o-mini\n",
    "    delay_in_seconds=5,       # 10 ist sehr sicher, 5 reicht meistens auch\n",
    "    exponential_backoff=True, \n",
    "    nr_docs=5,                # 5 Docs geben GPT genug Kontext für gute Labels\n",
    ")\n",
    "\n",
    "# 3. Topics aktualisieren\n",
    "topic_model_quanten.update_topics(docs, representation_model=rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a611ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_quanten.get_topic_info() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hex_topic_model_it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
